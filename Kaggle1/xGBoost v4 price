
auf price instead of logprice 

>> Refine Best: RMSE(price)=384.37 | RMSE(log)=0.51489 @ 445 rounds




############################################################
# Airbnb Chicago Price Prediction - Data Preparation + XGBoost
# Objective: Prepare features and train a regression model that
#            is selected by RMSE on the original price scale.
#            (We still track log-RMSE for analysis/tiebreaking.)
############################################################

# -------------------------------
# 1) Load Required Libraries
# -------------------------------
library(dplyr)        # Data wrangling
library(ggplot2)      # Exploratory plots
library(geosphere)    # Haversine distance
library(fastDummies)  # One-hot encoding for categorical features
library(reshape2)     # Correlation heatmap prep

# For modeling (later sections)
library(xgboost)      # Gradient boosting
library(caret)        # Quick model grids / CV helpers
library(Matrix)       # Sparse/dense matrix conversions
library(parallel)     # Parallel workers
library(doParallel)   # caret parallel backend

set.seed(42)          # Reproducibility across the entire pipeline

# -------------------------------
# 2) Load Data
# -------------------------------
# NOTE: Paths can be adjusted to your environment if needed.
train_raw <- read.csv("~/Documents/Uni/Master/Machine learning/mlai-2025-regression-challenge/train.csv")
test_raw  <- read.csv("~/Documents/Uni/Master/Machine learning/mlai-2025-regression-challenge/test.csv")

# Work on copies to keep originals intact
train <- train_raw
test  <- test_raw

cat("Training set summary:\n"); summary(train)
cat("\nTest set summary:\n");    summary(test)

# -------------------------------
# 3) Basic Data Cleaning
# -------------------------------
# Remove columns that do not provide predictive signal or would trivially leak IDs.
train <- train %>% select(-ID, -host_id, -name, -host_name)
test  <- test  %>% select(-ID, -host_id, -name, -host_name)

# NA overview (sanity check)
cat("\nMissing values per column (train):\n"); print(colSums(is.na(train)))
cat("\nMissing values per column (test):\n");  print(colSums(is.na(test)))

# Impute 'reviews_per_month' with 0 (interpretation: no reviews yet → effectively zero rate)
train$reviews_per_month[is.na(train$reviews_per_month)] <- 0
test$reviews_per_month[is.na(test$reviews_per_month)]   <- 0

cat("\nAfter imputation, missing values per column (train):\n"); print(colSums(is.na(train)))

# -------------------------------
# 4) Target Transformation
# -------------------------------
# We log-transform the target to stabilize variance and mitigate heavy right tails.
# NOTE: The assignment's evaluation is on the ORIGINAL price scale (RMSE on price),
#       so we will *select* the final model by RMSE(price) later, even though training
#       occurs on log(price). We keep both metrics for full transparency.
train <- train %>% mutate(log_price = log1p(price))

# -------------------------------
# 5) Geographic Feature: Distance to City Center (Chicago Loop)
# -------------------------------
# Coordinates for Chicago's main urban center (The Loop)
center_lon <- -87.6298
center_lat <- 41.8781

# Haversine distance in kilometers
train$dist_to_center <- distHaversine(cbind(train$longitude, train$latitude),
                                      c(center_lon, center_lat)) / 1000
test$dist_to_center  <- distHaversine(cbind(test$longitude,  test$latitude),
                                      c(center_lon, center_lat)) / 1000

cat("\nDistance-to-center summary (train):\n"); summary(train$dist_to_center)

# 5.1 Distance derivatives & proximity rings (nonlinear patterns wrt distance)
add_distance_derivatives <- function(df) {
  df$log_dist_to_center <- log1p(df$dist_to_center)      # diminishing returns
  df$dist2_to_center    <- df$dist_to_center^2           # allow quadratic curvature
  for (r in c(2, 5, 10)) {                               # coarse proximity bands
    df[[paste0("within_", r, "km")]] <- as.integer(df$dist_to_center <= r)
  }
  df
}
train <- add_distance_derivatives(train)
test  <- add_distance_derivatives(test)

# -------------------------------
# 6) Additional Feature Engineering
# -------------------------------
# Ratios/logs to stabilize skew and approximate linear additivity for linear parts,
# while still benefiting tree-based models (robust to monotonic transforms).
train <- train %>%
  mutate(
    reviews_ratio          = number_of_reviews / (reviews_per_month + 1),
    min_nights_log         = log1p(minimum_nights),
    number_of_reviews_log  = log1p(number_of_reviews),
    reviews_per_month_log  = log1p(reviews_per_month),
    availability_365_log   = log1p(availability_365),
    calc_host_listings_log = log1p(calculated_host_listings_count)
  )

test <- test %>%
  mutate(
    reviews_ratio          = number_of_reviews / (reviews_per_month + 1),
    min_nights_log         = log1p(minimum_nights),
    number_of_reviews_log  = log1p(number_of_reviews),
    reviews_per_month_log  = log1p(reviews_per_month),
    availability_365_log   = log1p(availability_365),
    calc_host_listings_log = log1p(calculated_host_listings_count)
  )

cat("\nCheck NAs after feature engineering (train):\n"); print(colSums(is.na(train)))

# -------------------------------
# 7) Encode Categorical: Room Type (One-Hot)
# -------------------------------
train <- dummy_cols(train, select_columns = "room_type", remove_selected_columns = TRUE)
test  <- dummy_cols(test,  select_columns = "room_type", remove_selected_columns = TRUE)

cat("\nEncoded room_type columns in train:\n")
print(names(train)[grepl("room_type_", names(train))])

# Ensure train/test have identical feature columns after encoding;
# IMPORTANT: do NOT include 'price' or 'log_price' in this alignment.
align_columns <- function(train_df, test_df) {
  missing_cols <- setdiff(names(train_df), names(test_df))
  for (col in missing_cols) test_df[[col]] <- 0
  extra_cols <- setdiff(names(test_df), names(train_df))
  if (length(extra_cols) > 0) test_df <- test_df[, setdiff(names(test_df), extra_cols)]
  test_df <- test_df[, names(train_df)]
  test_df
}
tmp_train_X <- train %>% dplyr::select(-price, -log_price)
test <- align_columns(tmp_train_X, test)

# Interactions: distance × room type to allow different distance-price slopes by segment
private_col <- grep("^room_type[_.]Private", names(train), value = TRUE)[1]
entire_col  <- grep("^room_type[_.]Entire",  names(train), value = TRUE)[1]
shared_col  <- grep("^room_type[_.]Shared",  names(train), value = TRUE)[1]

if (!is.na(private_col)) {
  train$dist_x_private <- train$dist_to_center * as.integer(train[[private_col]] == 1)
  test$dist_x_private  <- test$dist_to_center  * as.integer(test[[private_col]]  == 1)
}
if (!is.na(entire_col)) {
  train$dist_x_entire <- train$dist_to_center * as.integer(train[[entire_col]] == 1)
  test$dist_x_entire  <- test$dist_to_center  * as.integer(test[[entire_col]]  == 1)
}
if (!is.na(shared_col)) {
  train$dist_x_shared <- train$dist_to_center * as.integer(train[[shared_col]] == 1)
  test$dist_x_shared  <- test$dist_to_center  * as.integer(test[[shared_col]]  == 1)
}

# -------------------------------
# 8) Target Encoding (OOF) to avoid leakage
# -------------------------------
# We perform out-of-fold target encoding for high-cardinality location signals:
# (a) neighbourhood (categorical) and (b) geo micro-clusters (k-means on lon/lat).
# OOF ensures that each row's encoded value is computed without peeking at its own target.

oof_target_encode <- function(df, col, target, k = 5, alpha = 20, seed = 42) {
  set.seed(seed)
  folds <- sample(rep(1:k, length.out = nrow(df)))
  global <- mean(df[[target]], na.rm = TRUE)
  
  enc_vec <- numeric(nrow(df))
  for (f in 1:k) {
    tr <- df[folds != f, , drop = FALSE]
    te_idx <- which(folds == f)
    
    stats <- tr |>
      dplyr::group_by(.data[[col]]) |>
      dplyr::summarise(n = dplyr::n(),
                       m = mean(.data[[target]], na.rm = TRUE),
                       .groups = "drop") |>
      dplyr::mutate(m_smooth = (n*m + alpha*global) / (n + alpha)) |>
      dplyr::select(dplyr::all_of(col), m_smooth)
    
    joined <- df[te_idx, c(col), drop = FALSE] |>
      dplyr::left_join(stats, by = col)
    
    enc_vec[te_idx] <- ifelse(is.na(joined$m_smooth), global, joined$m_smooth)
  }
  
  # Mapping for test/new data:
  mapping <- df |>
    dplyr::group_by(.data[[col]]) |>
    dplyr::summarise(n = dplyr::n(),
                     m = mean(.data[[target]], na.rm = TRUE),
                     .groups = "drop") |>
    dplyr::mutate(m_smooth = (n*m + alpha*global) / (n + alpha)) |>
    dplyr::select(dplyr::all_of(col), m_smooth)
  
  list(oof = enc_vec, mapping = mapping, global = global)
}

# 8.0a Frequency of 'neighbourhood' (size/popularity proxy; computed on train only)
neigh_count <- train %>%
  dplyr::group_by(neighbourhood) %>%
  dplyr::summarise(neigh_n = dplyr::n(), .groups = "drop")

train <- train %>% dplyr::left_join(neigh_count, by = "neighbourhood")
test  <- test  %>% dplyr::left_join(neigh_count, by = "neighbourhood")
test$neigh_n[is.na(test$neigh_n)] <- 0  # unseen neighbourhoods in test → 0

# 8.1 OOF target encoding for 'neighbourhood'
train$neighbourhood <- as.character(train$neighbourhood)
test$neighbourhood  <- as.character(test$neighbourhood)

enc <- oof_target_encode(train, col = "neighbourhood", target = "log_price",
                         k = 5, alpha = 20, seed = 42)
train$neigh_te <- enc$oof

test <- test |> dplyr::left_join(enc$mapping, by = "neighbourhood")
test$neigh_te <- ifelse(is.na(test$m_smooth), enc$global, test$m_smooth)

# Drop raw 'neighbourhood' (keep only encoded version to reduce dimensionality)
train <- dplyr::select(train, -neighbourhood)
test  <- dplyr::select(test,  -neighbourhood, -m_smooth)

cat("\nOOF target-encoded 'neigh_te' summary (train):\n"); summary(train$neigh_te)

# 8.0b Geo micro-clusters (k-means on standardized lon/lat) + OOF target encoding
set.seed(42)
xy_train    <- as.matrix(train[, c("longitude","latitude")])
xy_train_sc <- scale(xy_train)
sc_center   <- attr(xy_train_sc, "scaled:center")
sc_scale    <- attr(xy_train_sc, "scaled:scale")

km <- kmeans(xy_train_sc, centers = 20, nstart = 20)  # moderate granularity
train$geo_cluster <- factor(km$cluster)

# Assign test rows to nearest centroid in the same standardized space
xy_test    <- as.matrix(test[, c("longitude","latitude")])
xy_test_sc <- scale(xy_test, center = sc_center, scale = sc_scale)
centers    <- km$centers
nearest_centroid <- function(x, centers) which.min(colSums((t(centers) - x)^2))
test$geo_cluster <- factor(apply(xy_test_sc, 1, nearest_centroid, centers = centers))

# OOF-encode the cluster id against log_price
enc_gc <- oof_target_encode(
  df = transform(train, gc = as.character(geo_cluster)),
  col = "gc", target = "log_price", k = 5, alpha = 20, seed = 42
)
train$geo_cluster_te <- enc_gc$oof
gc_map <- enc_gc$mapping; names(gc_map)[1] <- "gc"
tmp_gc <- dplyr::left_join(data.frame(gc = as.character(test$geo_cluster)),
                           gc_map, by = "gc")
test$geo_cluster_te <- ifelse(is.na(tmp_gc$m_smooth), enc_gc$global, tmp_gc$m_smooth)

# Keep only encoded version to keep the design matrix compact
train$geo_cluster <- NULL
test$geo_cluster  <- NULL

# -------------------------------
# 9) Temporal Features (review recency & seasonality)
# -------------------------------
# a) Days since last review (recency proxy) computed using TRAIN-ONLY reference date
train$last_review <- as.Date(train$last_review)
test$last_review  <- as.Date(test$last_review)

ref_date <- max(as.Date(train_raw$last_review), na.rm = TRUE)  # TRAIN ONLY reference
cat("\nReference date (most recent review in TRAIN):", ref_date, "\n")

train <- train %>% mutate(days_since_last_review = as.numeric(ref_date - last_review))
test  <- test  %>% mutate(days_since_last_review = as.numeric(ref_date - last_review))

# Missing last_review → set to the maximum observed days (train-only) to avoid peek
max_days_train <- max(train$days_since_last_review, na.rm = TRUE)
train$days_since_last_review[is.na(train$days_since_last_review)] <- max_days_train
test$days_since_last_review[is.na(test$days_since_last_review)]   <- max_days_train

# Log-transform for stability
train$log_days_since_last_review <- log1p(train$days_since_last_review)
test$log_days_since_last_review  <- log1p(test$days_since_last_review)

cat("\nSummary of 'days_since_last_review' (train):\n"); summary(train$days_since_last_review)

# b) Seasonality of the last review month via cyclical encoding (sin/cos)
train$review_month <- as.numeric(format(train$last_review, "%m"))
test$review_month  <- as.numeric(format(test$last_review,  "%m"))

# Listings without reviews → month = 0; use explicit indicator and set sin/cos to 0
train$review_month[is.na(train$review_month)] <- 0
test$review_month[is.na(test$review_month)]   <- 0

train$review_month_sin <- ifelse(train$review_month == 0, 0,
                                 sin(2 * pi * train$review_month / 12))
train$review_month_cos <- ifelse(train$review_month == 0, 0,
                                 cos(2 * pi * train$review_month / 12))
test$review_month_sin  <- ifelse(test$review_month == 0, 0,
                                 sin(2 * pi * test$review_month / 12))
test$review_month_cos  <- ifelse(test$review_month == 0, 0,
                                 cos(2 * pi * test$review_month / 12))

train$has_review            <- as.integer(!is.na(train$last_review))
test$has_review             <- as.integer(!is.na(test$last_review))
train$review_month_missing  <- as.integer(train$review_month == 0)
test$review_month_missing   <- as.integer(test$review_month == 0)

# -------------------------------
# 10) Remove Raw Date Columns (keep lat/lon for geo signal)
# -------------------------------
train <- dplyr::select(train, -last_review, -minimum_nights, -days_since_last_review)
test  <- dplyr::select(test,  -last_review, -minimum_nights, -days_since_last_review)

# -------------------------------
# 11) Final Integrity Checks
# -------------------------------
cat("\nFinal missing value counts (train):\n"); print(colSums(is.na(train)))
cat("\nFinal missing value counts (test):\n");  print(colSums(is.na(test)))
cat("\nFinal training columns:\n");              print(names(train))

# Correlation diagnostics (optional but useful for reporting)
numeric_features <- dplyr::select_if(train, is.numeric)
cor_matrix <- cor(numeric_features, use = "complete.obs")
cat("\nCorrelation of numerical features with log_price:\n")
print(sort(cor_matrix["log_price", ], decreasing = TRUE))

# cor.data map
 cor_data <- data.frame(
   feature = names(cor_matrix["log_price", ]),
   correlation = cor_matrix["log_price", ]
 ) %>%
   filter(feature != "log_price") %>%
   arrange(desc(abs(correlation)))
 ggplot(cor_data, aes(x = reorder(feature, correlation), y = correlation, fill = correlation > 0)) +
   geom_bar(stat = "identity", show.legend = FALSE) +
   coord_flip() +
   labs(title = "Correlation of Features with log_price",
        x = "Feature", y = "Pearson Correlation") +
   theme_minimal()

# Heatmap 
 cor_melt <- melt(cor_matrix); cor_melt$value <- round(cor_melt$value, 2)
 ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
   geom_tile(color = "white") +
   geom_text(aes(label = sprintf("%.2f", value)), size = 3) +
  scale_fill_gradient2(low = "firebrick", mid = "white", high = "steelblue",
                      midpoint = 0, limits = c(-1, 1)) +
  labs(title = "Feature Correlation Heatmap", x = "Feature", y = "Feature", fill = "Correlation") +
  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))

cat("\n[Info] Train rows/cols:", nrow(train), ncol(train), "\n")
cat("[Info] Test  rows/cols:", nrow(test),  ncol(test),  "\n")
common_cols <- intersect(names(train), names(test))
cat("[Info] Common feature columns (excl. target):",
    length(setdiff(common_cols, c("price","log_price"))), "\n")

# =====================================================================
# 12.5) FINAL SAFETY ALIGN + Feature Matrices for Modeling (CRITICAL)
# =====================================================================
# Ensure test and train share the exact same feature set (excluding targets),
# then build X/y matrices. This alignment before modeling avoids any mismatch.
test <- align_columns(train %>% dplyr::select(-price, -log_price), test)

X_train <- train %>% dplyr::select(-price, -log_price)
y_train <- train$log_price
X_test  <- test

############################################################
# 13–15) XGBoost — Fast Grid + Top-K Refine + Early Stopping
#         **MODEL SELECTION BY RMSE ON PRICE SCALE**
#         (log-RMSE is tracked and used as a tiebreaker only)
############################################################

# Parallel setup (fits stay single-threaded; CV/grid can use parallelism)
NUM_WORKERS <- max(1, parallel::detectCores() - 1)
cl <- parallel::makeCluster(NUM_WORKERS)
doParallel::registerDoParallel(cl)
on.exit(stopCluster(cl), add = TRUE)

# Prevent over-subscription inside xgboost fits
Sys.setenv(OMP_NUM_THREADS = "1", MKL_NUM_THREADS = "1")

# Helper: RMSE on original price scale from log-predictions
rmse_price <- function(y_log, yhat_log) {
  y    <- expm1(y_log)
  yhat <- expm1(yhat_log)
  sqrt(mean((yhat - y)^2))
}

# DMatrix for xgboost API
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)

# -------------------------------
# 14) Fast Grid via caret (5-fold CV)
# -------------------------------
# We use a compact yet effective grid to identify promising parameter regions.
# Metric here is RMSE on log-price (caret's xgbTree reports RMSE on label scale),
# which is fine for pre-selection. Final selection will happen by RMSE(price) below.
set.seed(42)
xgb_grid <- expand.grid(
  nrounds          = c(200, 320),
  max_depth        = c(6, 8),
  eta              = c(0.03),
  gamma            = c(0),
  colsample_bytree = c(0.75, 0.90),
  min_child_weight = c(3, 5),
  subsample        = c(0.75, 0.90)
)

train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  allowParallel = TRUE
)

xgb_tuned <- train(
  x = as.matrix(X_train),
  y = y_train,
  trControl = train_control,
  tuneGrid  = xgb_grid,
  method    = "xgbTree",
  metric    = "RMSE",      # caret returns RMSE on the label scale (here: log_price)
  nthread   = 1,
  tree_method = "hist"
)

cat("[Grid size]:", nrow(xgb_grid), "combinations; total fits =", nrow(xgb_grid) * 5, "\n")
cat("\n>> Fast-Grid bestTune & RMSE(log):\n"); print(xgb_tuned$bestTune); print(min(xgb_tuned$results$RMSE))

# -------------------------------
# 15) Top-K Refine with xgb.cv + Early Stopping
# -------------------------------
# We carry over the top candidates (by log-RMSE) into an xgb.cv refinement stage
# with EARLY STOPPING and FIXED FOLDS to compute out-of-fold predictions. We then
# compute RMSE on the PRICE SCALE using those OOF predictions and *select the best
# configuration by RMSE(price)*. If two configs are within $1, we prefer the lower
# log-RMSE as a principled tiebreaker.
topK <- xgb_tuned$results %>%
  arrange(RMSE) %>%
  dplyr::distinct(max_depth, eta, subsample, colsample_bytree, min_child_weight, gamma, .keep_all = TRUE) %>%
  head(6)

# Fixed folds for fair candidate comparison (identical splits for all parameter sets)
set.seed(42)
fold_ids <- sample(rep(1:5, length.out = nrow(X_train)))
folds    <- lapply(1:5, function(k) which(fold_ids == k))

refine_rows <- list()
best_overall <- list(score_price = Inf, score_log = Inf)
eps <- 1.0  # $-tolerance for tiebreaker on log-RMSE

for (i in seq_len(nrow(topK))) {
  row <- topK[i, ]
  
  # Refinement parameters inherit grid values and add mild L2 regularization (lambda)
  params_refine <- list(
    objective         = "reg:squarederror",
    eval_metric       = "rmse",
    eta               = row$eta,
    max_depth         = row$max_depth,
    subsample         = row$subsample,
    colsample_bytree  = row$colsample_bytree,
    min_child_weight  = row$min_child_weight,
    gamma             = row$gamma,
    lambda            = 1.0,   # mild L2 regularization for generalization robustness
    alpha             = 0.0,   # no L1 here (kept simple; can be tuned later)
    tree_method       = "hist",
    nthread           = 1
  )
  
  set.seed(42 + i)
  cv <- xgb.cv(
    params = params_refine,
    data   = dtrain,
    folds  = folds,                # fixed folds for comparability
    nrounds = 2500,
    early_stopping_rounds = 120,
    prediction = TRUE,             # returns OOF preds to compute RMSE(price)
    verbose = 0
  )
  
  score_log <- cv$evaluation_log$test_rmse_mean[cv$best_iteration]  # log-RMSE
  oof_price <- rmse_price(y_train, cv$pred)                          # RMSE on price scale
  
  refine_rows[[i]] <- data.frame(
    i = i,
    eta = row$eta,
    max_depth = row$max_depth,
    subsample = row$subsample,
    colsample_bytree = row$colsample_bytree,
    min_child_weight = row$min_child_weight,
    gamma = row$gamma,
    best_iteration = cv$best_iteration,
    rmse_log = score_log,
    oof_rmse_price = oof_price
  )
  
  # **ASSIGNMENT-METRIC SELECTION RULE**
  # Prefer lower RMSE(price). If nearly tied (<= $1), choose lower log-RMSE.
  if ( (oof_price < best_overall$score_price - 1e-9) ||
       (abs(oof_price - best_overall$score_price) <= eps && score_log < best_overall$score_log) ) {
    best_overall <- list(
      params      = params_refine,
      best_iter   = cv$best_iteration,
      score_price = oof_price,
      score_log   = score_log
    )
  }
}

refine_summary <- do.call(rbind, refine_rows)
cat("\n>> Refine Summary (Top-K):\n"); print(refine_summary)
cat(sprintf("\n>> Refine Best: RMSE(price)=%.2f | RMSE(log)=%.5f @ %d rounds\n",
            best_overall$score_price, best_overall$score_log, best_overall$best_iter))

# -------------------------------
# Final Training on full training set with best params/rounds
# -------------------------------
final_model <- xgb.train(
  params  = best_overall$params,
  data    = dtrain,
  nrounds = best_overall$best_iter,
  print_every_n = 50
)

# In-sample diagnostics (not used for model choice; just to inspect fit quality)
pred_train_log   <- predict(final_model, as.matrix(X_train))
rmse_log_train   <- sqrt(mean((pred_train_log - y_train)^2))
rmse_price_train <- rmse_price(y_train, pred_train_log)
cat(sprintf("\nTrain RMSE(log)=%.5f | RMSE(price)=%.2f\n",
            rmse_log_train, rmse_price_train))

# Feature importance (optional visualization; helps with interpretation/reporting)
importance <- xgb.importance(model = final_model)
xgb.plot.importance(importance, top_n = 15, rel_to_first = TRUE,
                    xlab = "Relative Importance",
                    main = "Final XGBoost Feature Importance")

# -------------------------------
# Test Predictions + Submission File
# -------------------------------
dtest <- xgb.DMatrix(data = as.matrix(X_test))
test_pred_log   <- predict(final_model, dtest)
test_pred_price <- expm1(test_pred_log)

submission <- data.frame(
  ID    = test_raw$ID,
  price = round(test_pred_price, 2)
)

out_file <- "submission_xgb_fastrefine.csv"
write.csv(submission, out_file, row.names = FALSE)
cat(sprintf("\n✅ Done. Submission saved as '%s'\n", out_file))
############################################################
