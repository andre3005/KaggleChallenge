

Ergebnis RSME = Best cross-validated RMSE:
> print(min(xgb_tuned$results$RMSE))
[1] 0.5123989



############################################################
# Airbnb Chicago Price Prediction - Data Preparation Script
# Goal: Prepare data for regression modeling (minimize RMSE)
############################################################

# ----- 1. Load Required Libraries -----

library(dplyr)
library(ggplot2)
library(geosphere)
library(fastDummies)


# ----- 2. Load Data -----

# Read training and test data
train_raw <- read.csv("~/Documents/Uni/Master/Machine learning/mlai-2025-regression-challenge/train.csv")
test_raw  <- read.csv("~/Documents/Uni/Master/Machine learning/mlai-2025-regression-challenge/test.csv")

# Create working copies to keep originals unchanged
train <- train_raw
test  <- test_raw

# Display basic structure of the datasets
cat("Training set summary:\n")
summary(train)
cat("\nTest set summary:\n")
summary(test)


# ----- 3. Basic Data Cleaning -----

# Remove columns that do not contain predictive information
train <- train %>% select(-ID, -host_id, -name, -host_name)
test  <- test  %>% select(-ID, -host_id, -name, -host_name)

# Check for missing values
cat("\nMissing values per column (train):\n")
print(colSums(is.na(train)))
cat("\nMissing values per column (test):\n")
print(colSums(is.na(test)))

# Replace missing values in 'reviews_per_month' with 0
# Interpretation: missing means no reviews yet
train$reviews_per_month[is.na(train$reviews_per_month)] <- 0
test$reviews_per_month[is.na(test$reviews_per_month)] <- 0

# Verify that missing values have been handled
cat("\nAfter imputation, missing values per column (train):\n")
print(colSums(is.na(train)))


# ----- 4. Variable Transformations -----

# Apply a log transformation to price to reduce the impact of extreme outliers
train <- train %>%
  mutate(log_price = log1p(price))


# ----- 5. Add Geographic Feature: Distance to City Center -----

# Coordinates of Chicago’s main urban center (The Loop)
center_lon <- -87.6298
center_lat <- 41.8781

# Calculate distance (in kilometers) to the city center using Haversine formula
train$dist_to_center <- distHaversine(
  cbind(train$longitude, train$latitude),
  c(center_lon, center_lat)
) / 1000

test$dist_to_center <- distHaversine(
  cbind(test$longitude, test$latitude),
  c(center_lon, center_lat)
) / 1000

# Check distance distribution to ensure plausible range
summary(train$dist_to_center)


# ----- 6. Additional Feature Engineering -----

# Create a review ratio (popularity measure) and log-transform minimum nights
train <- train %>%
  mutate(
    reviews_ratio = number_of_reviews / (reviews_per_month + 1),
    min_nights_log = log1p(minimum_nights)
  )

test <- test %>%
  mutate(
    reviews_ratio = number_of_reviews / (reviews_per_month + 1),
    min_nights_log = log1p(minimum_nights)
  )

# Additional robust log transforms for skewed count features
# Rationale: Make distributions more Gaussian-like for linear models and stabilize variance.
train <- train %>%
  mutate(
    number_of_reviews_log = log1p(number_of_reviews),
    reviews_per_month_log = log1p(reviews_per_month),
    availability_365_log = log1p(availability_365),
    calc_host_listings_log = log1p(calculated_host_listings_count)
  )

test <- test %>%
  mutate(
    number_of_reviews_log = log1p(number_of_reviews),
    reviews_per_month_log = log1p(reviews_per_month),
    availability_365_log = log1p(availability_365),
    calc_host_listings_log = log1p(calculated_host_listings_count)
  )

# Confirm that no new missing values were introduced
cat("\nCheck NAs after feature engineering:\n")
print(colSums(is.na(train)))


# ----- 7. Encode Room Type (One-Hot Encoding) -----

# Convert categorical variable 'room_type' into dummy columns (0/1)
train <- dummy_cols(train, select_columns = "room_type", remove_selected_columns = TRUE)
test  <- dummy_cols(test,  select_columns = "room_type", remove_selected_columns = TRUE)

# Confirm that encoding worked correctly
cat("\nEncoded room_type columns in train:\n")
print(names(train)[grepl("room_type_", names(train))])

# Check if train and test have matching column names
setdiff(names(train), names(test))

# ----- 7.1 Ensure train/test have identical columns after encoding -----
# Create any missing columns in test and reorder columns to match train.
align_columns <- function(train_df, test_df) {
  missing_cols <- setdiff(names(train_df), names(test_df))
  for (col in missing_cols) test_df[[col]] <- 0
  extra_cols <- setdiff(names(test_df), names(train_df))
  if (length(extra_cols) > 0) test_df <- test_df[, setdiff(names(test_df), extra_cols)]
  test_df <- test_df[, names(train_df)]
  test_df
}

# Do NOT include target columns in alignment
tmp_train_X <- train %>% dplyr::select(-price, -log_price)
test <- align_columns(tmp_train_X, test)

# ----- 7.2 Interaction: distance to center × room type (after dummies & alignment) -----
# We now safely have dummy columns like 'room_type_Private.room'.
# Be robust to naming: find a column that starts with room_type and contains "Private".
private_col <- grep("^room_type[_.]Private", names(train), value = TRUE)[1]

if (!is.na(private_col)) {
  train <- train %>%
    mutate(
      rt_private = as.integer(.data[[private_col]] == 1),
      dist_x_private = dist_to_center * rt_private
    )
  test <- test %>%
    mutate(
      rt_private = as.integer(.data[[private_col]] == 1),
      dist_x_private = dist_to_center * rt_private
    )
} else {
  warning("Private-room dummy not found; skipping interaction dist_to_center × room_type.")
}


# ----- 8 Helper: Out-of-Fold Target Encoding with smoothing -----
# Why: Prevents target leakage by computing per-neighbourhood means out-of-fold.
# alpha controls smoothing towards the global mean (helps for rare neighbourhoods).

oof_target_encode <- function(df, col, target, k = 5, alpha = 20, seed = 42) {
  set.seed(seed)
  folds <- sample(rep(1:k, length.out = nrow(df)))
  global <- mean(df[[target]], na.rm = TRUE)
  
  enc_vec <- numeric(nrow(df))
  for (f in 1:k) {
    tr <- df[folds != f, , drop = FALSE]
    te_idx <- which(folds == f)
    
    stats <- tr |>
      dplyr::group_by(.data[[col]]) |>
      dplyr::summarise(n = dplyr::n(),
                       m = mean(.data[[target]], na.rm = TRUE),
                       .groups = "drop") |>
      dplyr::mutate(m_smooth = (n*m + alpha*global) / (n + alpha)) |>
      dplyr::select(dplyr::all_of(col), m_smooth)
    
    joined <- df[te_idx, c(col), drop = FALSE] |>
      dplyr::left_join(stats, by = col)
    
    enc_vec[te_idx] <- ifelse(is.na(joined$m_smooth), global, joined$m_smooth)
  }
  
  # mapping for test / new data:
  mapping <- df |>
    dplyr::group_by(.data[[col]]) |>
    dplyr::summarise(n = dplyr::n(),
                     m = mean(.data[[target]], na.rm = TRUE),
                     .groups = "drop") |>
    dplyr::mutate(m_smooth = (n*m + alpha*global) / (n + alpha)) |>
    dplyr::select(dplyr::all_of(col), m_smooth)
  
  list(oof = enc_vec, mapping = mapping, global = global)
}


# ----- 8.1 Encode Neighbourhood (Out-of-Fold Target Encoding with smoothing) -----
# Motivation: Avoid leakage by computing neighbourhood means out-of-fold on train.
# We create 'neigh_te' for train via OOF, and map smoothed means to test.

# Ensure neighbourhood is present as character/factor:
train$neighbourhood <- as.character(train$neighbourhood)
test$neighbourhood  <- as.character(test$neighbourhood)

enc <- oof_target_encode(train, col = "neighbourhood", target = "log_price",
                         k = 5, alpha = 20, seed = 42)

train$neigh_te <- enc$oof

# Map to test set (unseen neighbourhoods -> global mean)
test <- test |>
  dplyr::left_join(enc$mapping, by = "neighbourhood")
test$neigh_te <- ifelse(is.na(test$m_smooth), enc$global, test$m_smooth)

# Drop original neighbourhood column (we keep the encoded version)
train <- dplyr::select(train, -neighbourhood)
test  <- dplyr::select(test,  -neighbourhood, -m_smooth)

# Quick sanity check
summary(train$neigh_te)


# ----- 9. Temporal Feature: Days Since Last Review -----

# Convert 'last_review' to Date format
train$last_review <- as.Date(train$last_review)
test$last_review  <- as.Date(test$last_review)

# Identify the most recent review date using TRAIN ONLY (avoid test peek)
ref_date <- max(as.Date(train_raw$last_review), na.rm = TRUE)
cat("\nReference date (most recent review in dataset):", ref_date, "\n")

# Calculate number of days since the last review
train <- train %>%
  mutate(days_since_last_review = as.numeric(ref_date - last_review))
test <- test %>%
  mutate(days_since_last_review = as.numeric(ref_date - last_review))

# Replace NAs (no reviews) using TRAIN-only maximum to avoid test peek
max_days_train <- max(train$days_since_last_review, na.rm = TRUE)
train$days_since_last_review[is.na(train$days_since_last_review)] <- max_days_train
test$days_since_last_review[is.na(test$days_since_last_review)]  <- max_days_train

# Log-transform for numerical stability
train$log_days_since_last_review <- log1p(train$days_since_last_review)
test$log_days_since_last_review  <- log1p(test$days_since_last_review)

# Basic summary for verification
cat("\nSummary of 'days_since_last_review':\n")
summary(train$days_since_last_review)

# Visualization of the distribution
ggplot(train, aes(x = days_since_last_review)) +
  geom_histogram(fill = "steelblue", bins = 30) +
  labs(title = "Distribution of Days Since Last Review",
       x = "Days since last review",
       y = "Count")


# ----- 10. Temporal Feature: Cyclical Encoding of Review Month -----
# Encode month of last review as cyclical features using sine and cosine transformation.
# This captures seasonal patterns smoothly for both linear and non-linear models.

# Extract review month (1–12)
train$review_month <- as.numeric(format(train$last_review, "%m"))
test$review_month  <- as.numeric(format(test$last_review, "%m"))

# Replace NAs (listings without reviews) with 0 → meaning no review
train$review_month[is.na(train$review_month)] <- 0
test$review_month[is.na(test$review_month)]   <- 0

# Apply cyclical encoding (handle month = 0 separately)
train$review_month_sin <- ifelse(train$review_month == 0, 0,
                                 sin(2 * pi * train$review_month / 12))
train$review_month_cos <- ifelse(train$review_month == 0, 0,
                                 cos(2 * pi * train$review_month / 12))

test$review_month_sin <- ifelse(test$review_month == 0, 0,
                                sin(2 * pi * test$review_month / 12))
test$review_month_cos <- ifelse(test$review_month == 0, 0,
                                cos(2 * pi * test$review_month / 12))

# Optional: visualize cyclical pattern
month_grid <- data.frame(
  month = 1:12,
  sin = sin(2 * pi * (1:12) / 12),
  cos = cos(2 * pi * (1:12) / 12)
)

ggplot(month_grid, aes(x = sin, y = cos, label = month)) +
  geom_point(color = "steelblue", size = 3) +
  geom_text(vjust = -1) +
  coord_equal() +
  labs(
    title = "Cyclical Encoding of Review Month (sin/cos)",
    x = "sin(2π * month / 12)",
    y = "cos(2π * month / 12)"
  ) +
  theme_minimal()


# ----- 11. Remove Original Date Columns (keep lat/lon for geo signal) -----
# Keep latitude & longitude (useful for tree models and for potential splines).
train <- dplyr::select(train, -last_review, -minimum_nights, -days_since_last_review)
test  <- dplyr::select(test,  -last_review, -minimum_nights, -days_since_last_review)


# ----- 12. Final Data Integrity Check -----

cat("\nFinal missing value counts (train):\n")
print(colSums(is.na(train)))
cat("\nFinal missing value counts (test):\n")
print(colSums(is.na(test)))

cat("\nFinal training columns:\n")
print(names(train))

# ----- Correlation Analysis: log_price vs. Numerical Features -----

# Select only numeric features
numeric_features <- select_if(train, is.numeric)

# Compute correlation matrix (complete cases only)
cor_matrix <- cor(numeric_features, use = "complete.obs")

# Print all correlations with log_price
cat("\nCorrelation of numerical features with log_price:\n")
print(sort(cor_matrix["log_price", ], decreasing = TRUE))

# --- Visualize correlations with a bar plot ---

# Convert correlations into a tidy dataframe for plotting
cor_data <- data.frame(
  feature = names(cor_matrix["log_price", ]),
  correlation = cor_matrix["log_price", ]
) %>%
  filter(feature != "log_price") %>%                # exclude self-correlation
  arrange(desc(abs(correlation)))                   # order by absolute correlation strength

# Plot correlation strength
ggplot(cor_data, aes(x = reorder(feature, correlation), y = correlation, fill = correlation > 0)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Correlation of Features with log_price",
    x = "Feature",
    y = "Pearson Correlation"
  ) +
  theme_minimal() +
  geom_text(aes(label = round(correlation, 2)), hjust = ifelse(cor_data$correlation > 0, -0.2, 1.2), size = 3) +
  scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "firebrick"))

# ----- Multicollinearity Check: Correlation Heatmap -----

library(reshape2)

# Compute correlation matrix for numeric features
cor_matrix <- cor(select_if(train, is.numeric), use = "complete.obs")

# Melt matrix into long format for ggplot
cor_melt <- melt(cor_matrix)

# Round correlation values to 2 decimal places for labeling
cor_melt$value <- round(cor_melt$value, 2)

# Plot correlation heatmap with text annotations
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +  # adds white gridlines
  geom_text(aes(label = sprintf("%.2f", value)), size = 3) +  # two decimals
  scale_fill_gradient2(
    low = "firebrick",
    mid = "white",
    high = "steelblue",
    midpoint = 0,
    limits = c(-1, 1)
  ) +
  labs(
    title = "Feature Correlation Heatmap (rounded to 2 decimals)",
    x = "Feature",
    y = "Feature",
    fill = "Correlation"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )

cat("\n[Info] Train rows/cols:", nrow(train), ncol(train), "\n")
cat("[Info] Test  rows/cols:", nrow(test),  ncol(test),  "\n")
common_cols <- intersect(names(train), names(test))
cat("[Info] Common feature columns (excl. target):", length(setdiff(common_cols, c("price","log_price"))), "\n")


############################################################

############################################################
# ----- 13. Model Training: XGBoost Regression (Full Train) -----
############################################################

library(xgboost)
library(caret)
library(Matrix)

# ----- Step 1: Prepare Data -----

# Features (X) and target (y)
X_train <- train %>% select(-price, -log_price)
y_train <- train$log_price

# Create X_test from the processed 'test' (must match columns of X_train)
X_test <- test

# Safety: align columns again (no target columns here)
align_columns_for_model <- function(train_X, test_X) {
  missing_cols <- setdiff(names(train_X), names(test_X))
  for (col in missing_cols) test_X[[col]] <- 0
  extra_cols <- setdiff(names(test_X), names(train_X))
  if (length(extra_cols) > 0) test_X <- test_X[, setdiff(names(test_X), extra_cols)]
  test_X <- test_X[, names(train_X)]
  test_X
}
X_test <- align_columns_for_model(X_train, X_test)

# Convert to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)

# ----- Step 2: Define model parameters -----

params <- list(
  objective = "reg:squarederror",   # regression loss
  eval_metric = "rmse",
  eta = 0.05,            # learning rate
  max_depth = 6,         # tree depth
  subsample = 0.8,       # row sampling
  colsample_bytree = 0.8,# feature sampling
  min_child_weight = 5,
  lambda = 1,
  alpha = 0
)

# ----- Step 3: Cross-Validation for performance estimate -----
# Add prediction=TRUE so we can compute OOF RMSE on the original price scale.
set.seed(42)
cv_model <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 2000,
  nfold = 5,                      # 5-fold cross-validation
  early_stopping_rounds = 50,
  verbose = 1,
  prediction = TRUE               # <-- enables cv_model$pred (OOF preds)
)

# --- Report CV metrics (recommended for the PDF) ---
cv_rmse_log    <- cv_model$evaluation_log$test_rmse_mean[cv_model$best_iteration]
cv_rmse_log_sd <- cv_model$evaluation_log$test_rmse_std[cv_model$best_iteration]
cat(sprintf("\nCV RMSE (log) @%d rounds: %.4f ± %.4f\n",
            cv_model$best_iteration, cv_rmse_log, cv_rmse_log_sd))
cat(sprintf("≈ relative error (approx): %.1f%%\n", (exp(cv_rmse_log)-1)*100))

# Optional but recommended: OOF RMSE on original price scale ($)
rmse_price <- function(y_log, yhat_log) {
  y <- expm1(y_log); yhat <- expm1(yhat_log)
  sqrt(mean((yhat - y)^2))
}
if (!is.null(cv_model$pred)) {
  oof_pred_log    <- cv_model$pred
  oof_rmse_price  <- rmse_price(y_train, oof_pred_log)
  cat(sprintf("OOF RMSE (price): %.2f\n", oof_rmse_price))
}

best_nrounds <- cv_model$best_iteration
cat("\nBest number of boosting rounds (from CV):", best_nrounds, "\n")

# ----- Step 4: Train final model on full training data -----

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  print_every_n = 50
)

# ----- Step 5: Evaluate model performance on training data -----

pred_train <- predict(xgb_model, as.matrix(X_train))
rmse_train <- sqrt(mean((pred_train - y_train)^2))
cat("\nTraining RMSE (log):", round(rmse_train, 4), "\n")

# Also report RMSE on original price scale (more interpretable)
rmse_train_price <- rmse_price(y_train, pred_train)
cat("Training RMSE (price):", round(rmse_train_price, 2), "\n")

# ----- Step 6: Feature Importance -----

importance <- xgb.importance(model = xgb_model)
xgb.plot.importance(importance, top_n = 15, rel_to_first = TRUE, xlab = "Relative Importance")

# ----- Step 7: Predict on Test Data -----

# IMPORTANT: use the column-aligned X_test, not raw 'test'
dtest <- xgb.DMatrix(data = as.matrix(X_test))
test_pred_log <- predict(xgb_model, dtest)
test_pred_price <- expm1(test_pred_log)

submission <- data.frame(
  ID = test_raw$ID,
  price = round(test_pred_price, 2)
)

write.csv(submission, "submission_xgboost_fulltrain.csv", row.names = FALSE)
cat("\n✅ XGBoost model training complete. Submission file saved as 'submission_xgboost_fulltrain.csv'\n")


############################################################
# ----- 14. XGBoost Hyperparameter Tuning (Grid Search) -----
############################################################

library(caret)

set.seed(42)

# Define search grid
xgb_grid <- expand.grid(
  nrounds = c(150, 250, 400),      # number of boosting rounds
  max_depth = c(4, 6, 8),          # tree depth
  eta = c(0.03, 0.05, 0.1),        # learning rate
  gamma = 0,                       # regularization term
  colsample_bytree = c(0.7, 0.9),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.7, 0.9)
)

# Set up 5-fold cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  allowParallel = TRUE
)

# Train models across grid
xgb_tuned <- train(
  x = as.matrix(X_train),
  y = y_train,
  trControl = train_control,
  tuneGrid = xgb_grid,
  method = "xgbTree",
  metric = "RMSE"
)

# ----- Results -----
cat("\nBest tuning parameters:\n")
print(xgb_tuned$bestTune)

cat("\nBest cross-validated RMSE:\n")
print(min(xgb_tuned$results$RMSE))

# Plot tuning results
plot(xgb_tuned)


############################################################
# ----- 15. Final XGBoost Model with Tuned Parameters -----
############################################################

library(xgboost)

# Ensure matrices use the aligned frames from Step 13
X_train_mat <- as.matrix(X_train)
X_test_mat  <- as.matrix(X_test)

dtrain <- xgb.DMatrix(data = X_train_mat, label = y_train)
dtest  <- xgb.DMatrix(data = X_test_mat)

# Final parameters from caret's bestTune (use your printed values)
# Example based on your result: min_child_weight = 3, nrounds = 400
best_params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.03,
  max_depth = 8,
  subsample = 0.9,
  colsample_bytree = 0.9,
  min_child_weight = 3,  # <- from bestTune
  gamma = 0
)

# ---- Option A: fixed nrounds from bestTune (simple & reproducible) ----
# final_nrounds <- 400

# ---- Option B (recommended): refine around bestTune with early stopping ----
set.seed(42)
cv_refine <- xgb.cv(
  params = best_params,
  data = dtrain,
  nrounds = 2000,
  nfold = 5,
  early_stopping_rounds = 100,
  verbose = 0
)
final_nrounds <- cv_refine$best_iteration
cat(sprintf("Refined best_nrounds (from best_params): %d\n", final_nrounds))

set.seed(42)
final_model <- xgb.train(
  params = best_params,
  data = dtrain,
  nrounds = final_nrounds,    # use refined value (or 400 from Option A)
  verbose = 1
)

# ----- Predict on Test Set -----
pred_log <- predict(final_model, dtest)
pred_price <- expm1(pred_log)  # reverse log1p

# ----- Optional: Feature Importance -----
importance <- xgb.importance(model = final_model)
xgb.plot.importance(importance, top_n = 15, rel_to_first = TRUE,
                    main = "Final XGBoost Feature Importance")
